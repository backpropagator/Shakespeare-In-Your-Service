{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Model.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "mqRpOPW_XXPk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import glob\n",
        "import os\n",
        "import unicodedata\n",
        "import string\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import random\n",
        "import time\n",
        "import math\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2NuOgSAHfJDE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7fbd9905-5e69-4801-ebeb-e4f861edaa32"
      },
      "source": [
        "is_cuda = torch.cuda.is_available()\n",
        "\n",
        "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
        "if is_cuda:\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"GPU is available\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"GPU not available, CPU used\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU is available\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EPxlQa-9XXPr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "3739aea5-78e9-4225-8f42-fae1622e9a13"
      },
      "source": [
        "all_letters = string.ascii_letters + \" .,;:'!-?)(\"\n",
        "n_letters = len(all_letters)\n",
        "\n",
        "MAX_LENGTH = 15\n",
        "\n",
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "        and c in all_letters\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "def readfile(filename):\n",
        "    file = open(filename, encoding='utf-8').read().replace(\"\\n\",\" \")\n",
        "    return file\n",
        "\n",
        "data = readfile(\"/content/sample_data/Shakespere.txt\")\n",
        "#print(data)\n",
        "\n",
        "X = []\n",
        "y = []\n",
        "\n",
        "for i in range(0, len(data)-MAX_LENGTH):\n",
        "    X.append(data[i:i+MAX_LENGTH-1])\n",
        "    y.append(data[i+1:i+MAX_LENGTH])\n",
        "\n",
        "print(\"Number of Training Example: {}\".format(len(X)))\n",
        "print(\"\\nA Random Example:\")\n",
        "q = random.randint(0,len(X))\n",
        "print(\"Input: {}\\nOutput: {}\".format(X[q],y[q]))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of Training Example: 94198\n",
            "\n",
            "A Random Example:\n",
            "Input: ou teachest ho\n",
            "Output: u teachest how\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "okY-sN6OXXPx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "int2char = dict(enumerate(all_letters))\n",
        "char2int = {c : i for i, c in int2char.items()}"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uP0iWGJyXXP2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "bd9b19b4-7652-42da-ef52-da61c92344e5"
      },
      "source": [
        "for i in range(len(X)):\n",
        "    X[i] = [char2int[c] for c in X[i]]\n",
        "    y[i] = [char2int[c] for c in y[i]]\n",
        "\n",
        "print(\"\\nA Random Example:\")\n",
        "q = random.randint(0,len(X))\n",
        "print(\"Input: {}\\nOutput: {}\".format(X[q],y[q]))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "A Random Example:\n",
            "Input: [54, 52, 45, 7, 0, 19, 52, 14, 21, 4, 17, 59, 6, 14]\n",
            "Output: [52, 45, 7, 0, 19, 52, 14, 21, 4, 17, 59, 6, 14, 4]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmlGgtKEXXP6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dict_size = len(char2int)\n",
        "seq_len = MAX_LENGTH-1\n",
        "batch_size = len(X)\n",
        "\n",
        "def one_hot_encode(sequence, dict_size, seq_len, batch_size):\n",
        "    features = np.zeros((batch_size, seq_len, dict_size), dtype=np.float32)\n",
        "    \n",
        "    for i in range(batch_size):\n",
        "        for u in range(seq_len):\n",
        "            #print(i, u, sequence[i][u])\n",
        "            features[i, u, sequence[i][u] ] = 1\n",
        "    return features"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmxIpLbGXXP-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = one_hot_encode(X, dict_size, seq_len, batch_size)\n",
        "#y = one_hot_encode(y, dict_size, seq_len, batch_size)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9puEJRLXXQG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = torch.from_numpy(X).cuda()\n",
        "y = torch.tensor(y).cuda()"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ogkjo_cKXXQO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, n_layers):\n",
        "        super(RNN,self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.n_layers = n_layers\n",
        "        \n",
        "        self.rnn = nn.RNN(input_size, hidden_size, n_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "    \n",
        "    def forward(self, input):\n",
        "        batch_size = input.size(0)\n",
        "        \n",
        "        hidden = self.initHidden(batch_size)\n",
        "        \n",
        "        out, hidden = self.rnn(input, hidden)\n",
        "        \n",
        "        out = out.contiguous().view(-1, self.hidden_size)\n",
        "        out = self.fc(out)\n",
        "        \n",
        "        return out, hidden\n",
        "    \n",
        "    def initHidden(self, batch_size):\n",
        "        return torch.zeros(self.n_layers, batch_size, self.hidden_size).cuda()"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qSWNlgvAXXQS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rnn = RNN(dict_size, 128, dict_size, n_layers=3)\n",
        "rnn.cuda()\n",
        "n_epochs = 5000\n",
        "lr = 0.001\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(rnn.parameters(), lr=lr)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uHETMMntXXQZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9d7ecb47-a84c-459d-a150-5d20d0b2ba98"
      },
      "source": [
        "for epoch in range(1, n_epochs+1):\n",
        "    optimizer.zero_grad()\n",
        "    X.cuda()\n",
        "    y.cuda()\n",
        "    output, hidden = rnn(X)\n",
        "    #print(output)\n",
        "    loss = criterion( output, y.view(-1).long() )\n",
        "    #print(loss)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    if epoch%10 == 0:\n",
        "        print('Epoch: {}/{}.............'.format(epoch, n_epochs), end=' ')\n",
        "        print(\"Loss: {:.4f}\".format(loss.item()))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 10/5000............. Loss: 3.2454\n",
            "Epoch: 20/5000............. Loss: 3.0597\n",
            "Epoch: 30/5000............. Loss: 3.0304\n",
            "Epoch: 40/5000............. Loss: 3.0229\n",
            "Epoch: 50/5000............. Loss: 3.0177\n",
            "Epoch: 60/5000............. Loss: 3.0144\n",
            "Epoch: 70/5000............. Loss: 3.0092\n",
            "Epoch: 80/5000............. Loss: 2.9924\n",
            "Epoch: 90/5000............. Loss: 2.9454\n",
            "Epoch: 100/5000............. Loss: 2.8663\n",
            "Epoch: 110/5000............. Loss: 2.7799\n",
            "Epoch: 120/5000............. Loss: 2.7100\n",
            "Epoch: 130/5000............. Loss: 2.6485\n",
            "Epoch: 140/5000............. Loss: 2.5885\n",
            "Epoch: 150/5000............. Loss: 2.5350\n",
            "Epoch: 160/5000............. Loss: 2.4883\n",
            "Epoch: 170/5000............. Loss: 2.4475\n",
            "Epoch: 180/5000............. Loss: 2.4074\n",
            "Epoch: 190/5000............. Loss: 2.3695\n",
            "Epoch: 200/5000............. Loss: 2.3354\n",
            "Epoch: 210/5000............. Loss: 2.3026\n",
            "Epoch: 220/5000............. Loss: 2.2698\n",
            "Epoch: 230/5000............. Loss: 2.2378\n",
            "Epoch: 240/5000............. Loss: 2.2079\n",
            "Epoch: 250/5000............. Loss: 2.1798\n",
            "Epoch: 260/5000............. Loss: 2.1531\n",
            "Epoch: 270/5000............. Loss: 2.1322\n",
            "Epoch: 280/5000............. Loss: 2.1050\n",
            "Epoch: 290/5000............. Loss: 2.0825\n",
            "Epoch: 300/5000............. Loss: 2.0608\n",
            "Epoch: 310/5000............. Loss: 2.0406\n",
            "Epoch: 320/5000............. Loss: 2.0212\n",
            "Epoch: 330/5000............. Loss: 2.0026\n",
            "Epoch: 340/5000............. Loss: 1.9850\n",
            "Epoch: 350/5000............. Loss: 1.9676\n",
            "Epoch: 360/5000............. Loss: 1.9529\n",
            "Epoch: 370/5000............. Loss: 1.9363\n",
            "Epoch: 380/5000............. Loss: 1.9211\n",
            "Epoch: 390/5000............. Loss: 1.9069\n",
            "Epoch: 400/5000............. Loss: 1.8940\n",
            "Epoch: 410/5000............. Loss: 1.8808\n",
            "Epoch: 420/5000............. Loss: 1.8680\n",
            "Epoch: 430/5000............. Loss: 1.8567\n",
            "Epoch: 440/5000............. Loss: 1.8451\n",
            "Epoch: 450/5000............. Loss: 1.8335\n",
            "Epoch: 460/5000............. Loss: 1.8230\n",
            "Epoch: 470/5000............. Loss: 1.8127\n",
            "Epoch: 480/5000............. Loss: 1.8022\n",
            "Epoch: 490/5000............. Loss: 1.7929\n",
            "Epoch: 500/5000............. Loss: 1.7827\n",
            "Epoch: 510/5000............. Loss: 1.7738\n",
            "Epoch: 520/5000............. Loss: 1.7643\n",
            "Epoch: 530/5000............. Loss: 1.7551\n",
            "Epoch: 540/5000............. Loss: 1.7464\n",
            "Epoch: 550/5000............. Loss: 1.7379\n",
            "Epoch: 560/5000............. Loss: 1.7292\n",
            "Epoch: 570/5000............. Loss: 1.7225\n",
            "Epoch: 580/5000............. Loss: 1.7132\n",
            "Epoch: 590/5000............. Loss: 1.7057\n",
            "Epoch: 600/5000............. Loss: 1.6977\n",
            "Epoch: 610/5000............. Loss: 1.6901\n",
            "Epoch: 620/5000............. Loss: 1.6827\n",
            "Epoch: 630/5000............. Loss: 1.6754\n",
            "Epoch: 640/5000............. Loss: 1.6683\n",
            "Epoch: 650/5000............. Loss: 1.6628\n",
            "Epoch: 660/5000............. Loss: 1.6573\n",
            "Epoch: 670/5000............. Loss: 1.6502\n",
            "Epoch: 680/5000............. Loss: 1.6435\n",
            "Epoch: 690/5000............. Loss: 1.6370\n",
            "Epoch: 700/5000............. Loss: 1.6311\n",
            "Epoch: 710/5000............. Loss: 1.6252\n",
            "Epoch: 720/5000............. Loss: 1.6194\n",
            "Epoch: 730/5000............. Loss: 1.6138\n",
            "Epoch: 740/5000............. Loss: 1.6082\n",
            "Epoch: 750/5000............. Loss: 1.6026\n",
            "Epoch: 760/5000............. Loss: 1.5972\n",
            "Epoch: 770/5000............. Loss: 1.5918\n",
            "Epoch: 780/5000............. Loss: 1.5864\n",
            "Epoch: 790/5000............. Loss: 1.5874\n",
            "Epoch: 800/5000............. Loss: 1.5781\n",
            "Epoch: 810/5000............. Loss: 1.5718\n",
            "Epoch: 820/5000............. Loss: 1.5664\n",
            "Epoch: 830/5000............. Loss: 1.5615\n",
            "Epoch: 840/5000............. Loss: 1.5566\n",
            "Epoch: 850/5000............. Loss: 1.5520\n",
            "Epoch: 860/5000............. Loss: 1.5474\n",
            "Epoch: 870/5000............. Loss: 1.5431\n",
            "Epoch: 880/5000............. Loss: 1.5380\n",
            "Epoch: 890/5000............. Loss: 1.5336\n",
            "Epoch: 900/5000............. Loss: 1.5296\n",
            "Epoch: 910/5000............. Loss: 1.5246\n",
            "Epoch: 920/5000............. Loss: 1.5217\n",
            "Epoch: 930/5000............. Loss: 1.5166\n",
            "Epoch: 940/5000............. Loss: 1.5119\n",
            "Epoch: 950/5000............. Loss: 1.5077\n",
            "Epoch: 960/5000............. Loss: 1.5034\n",
            "Epoch: 970/5000............. Loss: 1.4994\n",
            "Epoch: 980/5000............. Loss: 1.4956\n",
            "Epoch: 990/5000............. Loss: 1.4915\n",
            "Epoch: 1000/5000............. Loss: 1.4887\n",
            "Epoch: 1010/5000............. Loss: 1.4835\n",
            "Epoch: 1020/5000............. Loss: 1.4796\n",
            "Epoch: 1030/5000............. Loss: 1.4757\n",
            "Epoch: 1040/5000............. Loss: 1.4736\n",
            "Epoch: 1050/5000............. Loss: 1.4688\n",
            "Epoch: 1060/5000............. Loss: 1.4646\n",
            "Epoch: 1070/5000............. Loss: 1.4609\n",
            "Epoch: 1080/5000............. Loss: 1.4584\n",
            "Epoch: 1090/5000............. Loss: 1.4555\n",
            "Epoch: 1100/5000............. Loss: 1.4505\n",
            "Epoch: 1110/5000............. Loss: 1.4468\n",
            "Epoch: 1120/5000............. Loss: 1.4432\n",
            "Epoch: 1130/5000............. Loss: 1.4396\n",
            "Epoch: 1140/5000............. Loss: 1.4364\n",
            "Epoch: 1150/5000............. Loss: 1.4329\n",
            "Epoch: 1160/5000............. Loss: 1.4305\n",
            "Epoch: 1170/5000............. Loss: 1.4273\n",
            "Epoch: 1180/5000............. Loss: 1.4233\n",
            "Epoch: 1190/5000............. Loss: 1.4197\n",
            "Epoch: 1200/5000............. Loss: 1.4166\n",
            "Epoch: 1210/5000............. Loss: 1.4142\n",
            "Epoch: 1220/5000............. Loss: 1.4102\n",
            "Epoch: 1230/5000............. Loss: 1.4074\n",
            "Epoch: 1240/5000............. Loss: 1.4051\n",
            "Epoch: 1250/5000............. Loss: 1.4016\n",
            "Epoch: 1260/5000............. Loss: 1.3980\n",
            "Epoch: 1270/5000............. Loss: 1.3949\n",
            "Epoch: 1280/5000............. Loss: 1.3940\n",
            "Epoch: 1290/5000............. Loss: 1.3897\n",
            "Epoch: 1300/5000............. Loss: 1.3858\n",
            "Epoch: 1310/5000............. Loss: 1.3839\n",
            "Epoch: 1320/5000............. Loss: 1.3809\n",
            "Epoch: 1330/5000............. Loss: 1.3783\n",
            "Epoch: 1340/5000............. Loss: 1.3744\n",
            "Epoch: 1350/5000............. Loss: 1.3718\n",
            "Epoch: 1360/5000............. Loss: 1.3711\n",
            "Epoch: 1370/5000............. Loss: 1.3669\n",
            "Epoch: 1380/5000............. Loss: 1.3638\n",
            "Epoch: 1390/5000............. Loss: 1.3607\n",
            "Epoch: 1400/5000............. Loss: 1.3615\n",
            "Epoch: 1410/5000............. Loss: 1.3565\n",
            "Epoch: 1420/5000............. Loss: 1.3530\n",
            "Epoch: 1430/5000............. Loss: 1.3503\n",
            "Epoch: 1440/5000............. Loss: 1.3481\n",
            "Epoch: 1450/5000............. Loss: 1.3464\n",
            "Epoch: 1460/5000............. Loss: 1.3428\n",
            "Epoch: 1470/5000............. Loss: 1.3403\n",
            "Epoch: 1480/5000............. Loss: 1.3377\n",
            "Epoch: 1490/5000............. Loss: 1.3357\n",
            "Epoch: 1500/5000............. Loss: 1.3338\n",
            "Epoch: 1510/5000............. Loss: 1.3311\n",
            "Epoch: 1520/5000............. Loss: 1.3283\n",
            "Epoch: 1530/5000............. Loss: 1.3257\n",
            "Epoch: 1540/5000............. Loss: 1.3245\n",
            "Epoch: 1550/5000............. Loss: 1.3211\n",
            "Epoch: 1560/5000............. Loss: 1.3189\n",
            "Epoch: 1570/5000............. Loss: 1.3191\n",
            "Epoch: 1580/5000............. Loss: 1.3150\n",
            "Epoch: 1590/5000............. Loss: 1.3117\n",
            "Epoch: 1600/5000............. Loss: 1.3102\n",
            "Epoch: 1610/5000............. Loss: 1.3076\n",
            "Epoch: 1620/5000............. Loss: 1.3061\n",
            "Epoch: 1630/5000............. Loss: 1.3032\n",
            "Epoch: 1640/5000............. Loss: 1.3014\n",
            "Epoch: 1650/5000............. Loss: 1.2998\n",
            "Epoch: 1660/5000............. Loss: 1.2969\n",
            "Epoch: 1670/5000............. Loss: 1.2946\n",
            "Epoch: 1680/5000............. Loss: 1.2924\n",
            "Epoch: 1690/5000............. Loss: 1.2905\n",
            "Epoch: 1700/5000............. Loss: 1.2906\n",
            "Epoch: 1710/5000............. Loss: 1.2870\n",
            "Epoch: 1720/5000............. Loss: 1.2843\n",
            "Epoch: 1730/5000............. Loss: 1.2823\n",
            "Epoch: 1740/5000............. Loss: 1.2806\n",
            "Epoch: 1750/5000............. Loss: 1.2797\n",
            "Epoch: 1760/5000............. Loss: 1.2766\n",
            "Epoch: 1770/5000............. Loss: 1.2748\n",
            "Epoch: 1780/5000............. Loss: 1.2734\n",
            "Epoch: 1790/5000............. Loss: 1.2717\n",
            "Epoch: 1800/5000............. Loss: 1.2690\n",
            "Epoch: 1810/5000............. Loss: 1.2665\n",
            "Epoch: 1820/5000............. Loss: 1.2659\n",
            "Epoch: 1830/5000............. Loss: 1.2654\n",
            "Epoch: 1840/5000............. Loss: 1.2622\n",
            "Epoch: 1850/5000............. Loss: 1.2599\n",
            "Epoch: 1860/5000............. Loss: 1.2577\n",
            "Epoch: 1870/5000............. Loss: 1.2562\n",
            "Epoch: 1880/5000............. Loss: 1.2570\n",
            "Epoch: 1890/5000............. Loss: 1.2528\n",
            "Epoch: 1900/5000............. Loss: 1.2511\n",
            "Epoch: 1910/5000............. Loss: 1.2492\n",
            "Epoch: 1920/5000............. Loss: 1.2495\n",
            "Epoch: 1930/5000............. Loss: 1.2475\n",
            "Epoch: 1940/5000............. Loss: 1.2476\n",
            "Epoch: 1950/5000............. Loss: 1.2430\n",
            "Epoch: 1960/5000............. Loss: 1.2409\n",
            "Epoch: 1970/5000............. Loss: 1.2394\n",
            "Epoch: 1980/5000............. Loss: 1.2376\n",
            "Epoch: 1990/5000............. Loss: 1.2377\n",
            "Epoch: 2000/5000............. Loss: 1.2365\n",
            "Epoch: 2010/5000............. Loss: 1.2339\n",
            "Epoch: 2020/5000............. Loss: 1.2314\n",
            "Epoch: 2030/5000............. Loss: 1.2298\n",
            "Epoch: 2040/5000............. Loss: 1.2289\n",
            "Epoch: 2050/5000............. Loss: 1.2327\n",
            "Epoch: 2060/5000............. Loss: 1.2277\n",
            "Epoch: 2070/5000............. Loss: 1.2250\n",
            "Epoch: 2080/5000............. Loss: 1.2226\n",
            "Epoch: 2090/5000............. Loss: 1.2211\n",
            "Epoch: 2100/5000............. Loss: 1.2196\n",
            "Epoch: 2110/5000............. Loss: 1.2259\n",
            "Epoch: 2120/5000............. Loss: 1.2171\n",
            "Epoch: 2130/5000............. Loss: 1.2157\n",
            "Epoch: 2140/5000............. Loss: 1.2145\n",
            "Epoch: 2150/5000............. Loss: 1.2153\n",
            "Epoch: 2160/5000............. Loss: 1.2122\n",
            "Epoch: 2170/5000............. Loss: 1.2101\n",
            "Epoch: 2180/5000............. Loss: 1.2089\n",
            "Epoch: 2190/5000............. Loss: 1.2083\n",
            "Epoch: 2200/5000............. Loss: 1.2077\n",
            "Epoch: 2210/5000............. Loss: 1.2056\n",
            "Epoch: 2220/5000............. Loss: 1.2035\n",
            "Epoch: 2230/5000............. Loss: 1.2030\n",
            "Epoch: 2240/5000............. Loss: 1.2038\n",
            "Epoch: 2250/5000............. Loss: 1.2008\n",
            "Epoch: 2260/5000............. Loss: 1.1990\n",
            "Epoch: 2270/5000............. Loss: 1.2023\n",
            "Epoch: 2280/5000............. Loss: 1.1966\n",
            "Epoch: 2290/5000............. Loss: 1.1953\n",
            "Epoch: 2300/5000............. Loss: 1.1938\n",
            "Epoch: 2310/5000............. Loss: 1.1923\n",
            "Epoch: 2320/5000............. Loss: 1.1913\n",
            "Epoch: 2330/5000............. Loss: 1.1908\n",
            "Epoch: 2340/5000............. Loss: 1.1919\n",
            "Epoch: 2350/5000............. Loss: 1.1892\n",
            "Epoch: 2360/5000............. Loss: 1.1873\n",
            "Epoch: 2370/5000............. Loss: 1.1857\n",
            "Epoch: 2380/5000............. Loss: 1.1842\n",
            "Epoch: 2390/5000............. Loss: 1.1886\n",
            "Epoch: 2400/5000............. Loss: 1.1830\n",
            "Epoch: 2410/5000............. Loss: 1.1828\n",
            "Epoch: 2420/5000............. Loss: 1.1809\n",
            "Epoch: 2430/5000............. Loss: 1.1788\n",
            "Epoch: 2440/5000............. Loss: 1.1788\n",
            "Epoch: 2450/5000............. Loss: 1.1771\n",
            "Epoch: 2460/5000............. Loss: 1.1757\n",
            "Epoch: 2470/5000............. Loss: 1.1747\n",
            "Epoch: 2480/5000............. Loss: 1.1742\n",
            "Epoch: 2490/5000............. Loss: 1.1733\n",
            "Epoch: 2500/5000............. Loss: 1.1730\n",
            "Epoch: 2510/5000............. Loss: 1.1711\n",
            "Epoch: 2520/5000............. Loss: 1.1712\n",
            "Epoch: 2530/5000............. Loss: 1.1692\n",
            "Epoch: 2540/5000............. Loss: 1.1678\n",
            "Epoch: 2550/5000............. Loss: 1.1664\n",
            "Epoch: 2560/5000............. Loss: 1.1664\n",
            "Epoch: 2570/5000............. Loss: 1.1659\n",
            "Epoch: 2580/5000............. Loss: 1.1692\n",
            "Epoch: 2590/5000............. Loss: 1.1635\n",
            "Epoch: 2600/5000............. Loss: 1.1620\n",
            "Epoch: 2610/5000............. Loss: 1.1605\n",
            "Epoch: 2620/5000............. Loss: 1.1619\n",
            "Epoch: 2630/5000............. Loss: 1.1591\n",
            "Epoch: 2640/5000............. Loss: 1.1584\n",
            "Epoch: 2650/5000............. Loss: 1.1570\n",
            "Epoch: 2660/5000............. Loss: 1.1557\n",
            "Epoch: 2670/5000............. Loss: 1.1581\n",
            "Epoch: 2680/5000............. Loss: 1.1574\n",
            "Epoch: 2690/5000............. Loss: 1.1537\n",
            "Epoch: 2700/5000............. Loss: 1.1520\n",
            "Epoch: 2710/5000............. Loss: 1.1511\n",
            "Epoch: 2720/5000............. Loss: 1.1522\n",
            "Epoch: 2730/5000............. Loss: 1.1500\n",
            "Epoch: 2740/5000............. Loss: 1.1502\n",
            "Epoch: 2750/5000............. Loss: 1.1498\n",
            "Epoch: 2760/5000............. Loss: 1.1488\n",
            "Epoch: 2770/5000............. Loss: 1.1463\n",
            "Epoch: 2780/5000............. Loss: 1.1451\n",
            "Epoch: 2790/5000............. Loss: 1.1443\n",
            "Epoch: 2800/5000............. Loss: 1.1513\n",
            "Epoch: 2810/5000............. Loss: 1.1449\n",
            "Epoch: 2820/5000............. Loss: 1.1434\n",
            "Epoch: 2830/5000............. Loss: 1.1417\n",
            "Epoch: 2840/5000............. Loss: 1.1403\n",
            "Epoch: 2850/5000............. Loss: 1.1440\n",
            "Epoch: 2860/5000............. Loss: 1.1414\n",
            "Epoch: 2870/5000............. Loss: 1.1386\n",
            "Epoch: 2880/5000............. Loss: 1.1371\n",
            "Epoch: 2890/5000............. Loss: 1.1367\n",
            "Epoch: 2900/5000............. Loss: 1.1360\n",
            "Epoch: 2910/5000............. Loss: 1.1370\n",
            "Epoch: 2920/5000............. Loss: 1.1351\n",
            "Epoch: 2930/5000............. Loss: 1.1339\n",
            "Epoch: 2940/5000............. Loss: 1.1331\n",
            "Epoch: 2950/5000............. Loss: 1.1339\n",
            "Epoch: 2960/5000............. Loss: 1.1327\n",
            "Epoch: 2970/5000............. Loss: 1.1307\n",
            "Epoch: 2980/5000............. Loss: 1.1296\n",
            "Epoch: 2990/5000............. Loss: 1.1326\n",
            "Epoch: 3000/5000............. Loss: 1.1311\n",
            "Epoch: 3010/5000............. Loss: 1.1287\n",
            "Epoch: 3020/5000............. Loss: 1.1268\n",
            "Epoch: 3030/5000............. Loss: 1.1258\n",
            "Epoch: 3040/5000............. Loss: 1.1253\n",
            "Epoch: 3050/5000............. Loss: 1.1351\n",
            "Epoch: 3060/5000............. Loss: 1.1247\n",
            "Epoch: 3070/5000............. Loss: 1.1233\n",
            "Epoch: 3080/5000............. Loss: 1.1229\n",
            "Epoch: 3090/5000............. Loss: 1.1253\n",
            "Epoch: 3100/5000............. Loss: 1.1221\n",
            "Epoch: 3110/5000............. Loss: 1.1212\n",
            "Epoch: 3120/5000............. Loss: 1.1208\n",
            "Epoch: 3130/5000............. Loss: 1.1198\n",
            "Epoch: 3140/5000............. Loss: 1.1187\n",
            "Epoch: 3150/5000............. Loss: 1.1186\n",
            "Epoch: 3160/5000............. Loss: 1.1203\n",
            "Epoch: 3170/5000............. Loss: 1.1179\n",
            "Epoch: 3180/5000............. Loss: 1.1169\n",
            "Epoch: 3190/5000............. Loss: 1.1155\n",
            "Epoch: 3200/5000............. Loss: 1.1168\n",
            "Epoch: 3210/5000............. Loss: 1.1147\n",
            "Epoch: 3220/5000............. Loss: 1.1175\n",
            "Epoch: 3230/5000............. Loss: 1.1148\n",
            "Epoch: 3240/5000............. Loss: 1.1125\n",
            "Epoch: 3250/5000............. Loss: 1.1116\n",
            "Epoch: 3260/5000............. Loss: 1.1125\n",
            "Epoch: 3270/5000............. Loss: 1.1109\n",
            "Epoch: 3280/5000............. Loss: 1.1113\n",
            "Epoch: 3290/5000............. Loss: 1.1095\n",
            "Epoch: 3300/5000............. Loss: 1.1092\n",
            "Epoch: 3310/5000............. Loss: 1.1106\n",
            "Epoch: 3320/5000............. Loss: 1.1098\n",
            "Epoch: 3330/5000............. Loss: 1.1068\n",
            "Epoch: 3340/5000............. Loss: 1.1062\n",
            "Epoch: 3350/5000............. Loss: 1.1057\n",
            "Epoch: 3360/5000............. Loss: 1.1062\n",
            "Epoch: 3370/5000............. Loss: 1.1057\n",
            "Epoch: 3380/5000............. Loss: 1.1062\n",
            "Epoch: 3390/5000............. Loss: 1.1037\n",
            "Epoch: 3400/5000............. Loss: 1.1031\n",
            "Epoch: 3410/5000............. Loss: 1.1043\n",
            "Epoch: 3420/5000............. Loss: 1.1018\n",
            "Epoch: 3430/5000............. Loss: 1.1033\n",
            "Epoch: 3440/5000............. Loss: 1.1014\n",
            "Epoch: 3450/5000............. Loss: 1.1002\n",
            "Epoch: 3460/5000............. Loss: 1.1030\n",
            "Epoch: 3470/5000............. Loss: 1.0994\n",
            "Epoch: 3480/5000............. Loss: 1.1031\n",
            "Epoch: 3490/5000............. Loss: 1.0990\n",
            "Epoch: 3500/5000............. Loss: 1.0978\n",
            "Epoch: 3510/5000............. Loss: 1.0971\n",
            "Epoch: 3520/5000............. Loss: 1.0992\n",
            "Epoch: 3530/5000............. Loss: 1.0966\n",
            "Epoch: 3540/5000............. Loss: 1.0959\n",
            "Epoch: 3550/5000............. Loss: 1.0956\n",
            "Epoch: 3560/5000............. Loss: 1.0952\n",
            "Epoch: 3570/5000............. Loss: 1.0978\n",
            "Epoch: 3580/5000............. Loss: 1.0950\n",
            "Epoch: 3590/5000............. Loss: 1.0937\n",
            "Epoch: 3600/5000............. Loss: 1.0947\n",
            "Epoch: 3610/5000............. Loss: 1.0929\n",
            "Epoch: 3620/5000............. Loss: 1.0922\n",
            "Epoch: 3630/5000............. Loss: 1.0923\n",
            "Epoch: 3640/5000............. Loss: 1.0931\n",
            "Epoch: 3650/5000............. Loss: 1.0907\n",
            "Epoch: 3660/5000............. Loss: 1.0893\n",
            "Epoch: 3670/5000............. Loss: 1.0899\n",
            "Epoch: 3680/5000............. Loss: 1.0893\n",
            "Epoch: 3690/5000............. Loss: 1.0915\n",
            "Epoch: 3700/5000............. Loss: 1.0884\n",
            "Epoch: 3710/5000............. Loss: 1.0877\n",
            "Epoch: 3720/5000............. Loss: 1.0870\n",
            "Epoch: 3730/5000............. Loss: 1.0888\n",
            "Epoch: 3740/5000............. Loss: 1.0857\n",
            "Epoch: 3750/5000............. Loss: 1.0861\n",
            "Epoch: 3760/5000............. Loss: 1.0855\n",
            "Epoch: 3770/5000............. Loss: 1.0902\n",
            "Epoch: 3780/5000............. Loss: 1.0862\n",
            "Epoch: 3790/5000............. Loss: 1.0843\n",
            "Epoch: 3800/5000............. Loss: 1.0840\n",
            "Epoch: 3810/5000............. Loss: 1.0836\n",
            "Epoch: 3820/5000............. Loss: 1.0839\n",
            "Epoch: 3830/5000............. Loss: 1.0817\n",
            "Epoch: 3840/5000............. Loss: 1.0812\n",
            "Epoch: 3850/5000............. Loss: 1.0807\n",
            "Epoch: 3860/5000............. Loss: 1.0819\n",
            "Epoch: 3870/5000............. Loss: 1.0855\n",
            "Epoch: 3880/5000............. Loss: 1.0813\n",
            "Epoch: 3890/5000............. Loss: 1.0804\n",
            "Epoch: 3900/5000............. Loss: 1.0789\n",
            "Epoch: 3910/5000............. Loss: 1.0790\n",
            "Epoch: 3920/5000............. Loss: 1.0812\n",
            "Epoch: 3930/5000............. Loss: 1.0788\n",
            "Epoch: 3940/5000............. Loss: 1.0776\n",
            "Epoch: 3950/5000............. Loss: 1.0776\n",
            "Epoch: 3960/5000............. Loss: 1.0782\n",
            "Epoch: 3970/5000............. Loss: 1.0764\n",
            "Epoch: 3980/5000............. Loss: 1.0758\n",
            "Epoch: 3990/5000............. Loss: 1.0750\n",
            "Epoch: 4000/5000............. Loss: 1.0800\n",
            "Epoch: 4010/5000............. Loss: 1.0794\n",
            "Epoch: 4020/5000............. Loss: 1.0738\n",
            "Epoch: 4030/5000............. Loss: 1.0742\n",
            "Epoch: 4040/5000............. Loss: 1.0744\n",
            "Epoch: 4050/5000............. Loss: 1.0735\n",
            "Epoch: 4060/5000............. Loss: 1.0733\n",
            "Epoch: 4070/5000............. Loss: 1.0723\n",
            "Epoch: 4080/5000............. Loss: 1.0734\n",
            "Epoch: 4090/5000............. Loss: 1.0715\n",
            "Epoch: 4100/5000............. Loss: 1.0717\n",
            "Epoch: 4110/5000............. Loss: 1.0734\n",
            "Epoch: 4120/5000............. Loss: 1.0708\n",
            "Epoch: 4130/5000............. Loss: 1.0707\n",
            "Epoch: 4140/5000............. Loss: 1.0702\n",
            "Epoch: 4150/5000............. Loss: 1.0696\n",
            "Epoch: 4160/5000............. Loss: 1.0710\n",
            "Epoch: 4170/5000............. Loss: 1.0692\n",
            "Epoch: 4180/5000............. Loss: 1.0732\n",
            "Epoch: 4190/5000............. Loss: 1.0695\n",
            "Epoch: 4200/5000............. Loss: 1.0680\n",
            "Epoch: 4210/5000............. Loss: 1.0673\n",
            "Epoch: 4220/5000............. Loss: 1.0675\n",
            "Epoch: 4230/5000............. Loss: 1.0684\n",
            "Epoch: 4240/5000............. Loss: 1.0666\n",
            "Epoch: 4250/5000............. Loss: 1.0656\n",
            "Epoch: 4260/5000............. Loss: 1.0661\n",
            "Epoch: 4270/5000............. Loss: 1.0675\n",
            "Epoch: 4280/5000............. Loss: 1.0656\n",
            "Epoch: 4290/5000............. Loss: 1.0647\n",
            "Epoch: 4300/5000............. Loss: 1.0648\n",
            "Epoch: 4310/5000............. Loss: 1.0682\n",
            "Epoch: 4320/5000............. Loss: 1.0651\n",
            "Epoch: 4330/5000............. Loss: 1.0640\n",
            "Epoch: 4340/5000............. Loss: 1.0635\n",
            "Epoch: 4350/5000............. Loss: 1.0638\n",
            "Epoch: 4360/5000............. Loss: 1.0627\n",
            "Epoch: 4370/5000............. Loss: 1.0630\n",
            "Epoch: 4380/5000............. Loss: 1.0621\n",
            "Epoch: 4390/5000............. Loss: 1.0609\n",
            "Epoch: 4400/5000............. Loss: 1.0608\n",
            "Epoch: 4410/5000............. Loss: 1.0624\n",
            "Epoch: 4420/5000............. Loss: 1.0640\n",
            "Epoch: 4430/5000............. Loss: 1.0598\n",
            "Epoch: 4440/5000............. Loss: 1.0600\n",
            "Epoch: 4450/5000............. Loss: 1.0587\n",
            "Epoch: 4460/5000............. Loss: 1.0591\n",
            "Epoch: 4470/5000............. Loss: 1.0616\n",
            "Epoch: 4480/5000............. Loss: 1.0590\n",
            "Epoch: 4490/5000............. Loss: 1.0598\n",
            "Epoch: 4500/5000............. Loss: 1.0572\n",
            "Epoch: 4510/5000............. Loss: 1.0573\n",
            "Epoch: 4520/5000............. Loss: 1.0571\n",
            "Epoch: 4530/5000............. Loss: 1.0584\n",
            "Epoch: 4540/5000............. Loss: 1.0567\n",
            "Epoch: 4550/5000............. Loss: 1.0586\n",
            "Epoch: 4560/5000............. Loss: 1.0568\n",
            "Epoch: 4570/5000............. Loss: 1.0561\n",
            "Epoch: 4580/5000............. Loss: 1.0559\n",
            "Epoch: 4590/5000............. Loss: 1.0551\n",
            "Epoch: 4600/5000............. Loss: 1.0543\n",
            "Epoch: 4610/5000............. Loss: 1.0562\n",
            "Epoch: 4620/5000............. Loss: 1.0544\n",
            "Epoch: 4630/5000............. Loss: 1.0557\n",
            "Epoch: 4640/5000............. Loss: 1.0534\n",
            "Epoch: 4650/5000............. Loss: 1.0526\n",
            "Epoch: 4660/5000............. Loss: 1.0548\n",
            "Epoch: 4670/5000............. Loss: 1.0537\n",
            "Epoch: 4680/5000............. Loss: 1.0531\n",
            "Epoch: 4690/5000............. Loss: 1.0522\n",
            "Epoch: 4700/5000............. Loss: 1.0523\n",
            "Epoch: 4710/5000............. Loss: 1.0528\n",
            "Epoch: 4720/5000............. Loss: 1.0518\n",
            "Epoch: 4730/5000............. Loss: 1.0508\n",
            "Epoch: 4740/5000............. Loss: 1.0527\n",
            "Epoch: 4750/5000............. Loss: 1.0520\n",
            "Epoch: 4760/5000............. Loss: 1.0520\n",
            "Epoch: 4770/5000............. Loss: 1.0496\n",
            "Epoch: 4780/5000............. Loss: 1.0499\n",
            "Epoch: 4790/5000............. Loss: 1.0513\n",
            "Epoch: 4800/5000............. Loss: 1.0500\n",
            "Epoch: 4810/5000............. Loss: 1.0500\n",
            "Epoch: 4820/5000............. Loss: 1.0484\n",
            "Epoch: 4830/5000............. Loss: 1.0482\n",
            "Epoch: 4840/5000............. Loss: 1.0507\n",
            "Epoch: 4850/5000............. Loss: 1.0476\n",
            "Epoch: 4860/5000............. Loss: 1.0478\n",
            "Epoch: 4870/5000............. Loss: 1.0477\n",
            "Epoch: 4880/5000............. Loss: 1.0464\n",
            "Epoch: 4890/5000............. Loss: 1.0476\n",
            "Epoch: 4900/5000............. Loss: 1.0484\n",
            "Epoch: 4910/5000............. Loss: 1.0484\n",
            "Epoch: 4920/5000............. Loss: 1.0466\n",
            "Epoch: 4930/5000............. Loss: 1.0458\n",
            "Epoch: 4940/5000............. Loss: 1.0548\n",
            "Epoch: 4950/5000............. Loss: 1.0481\n",
            "Epoch: 4960/5000............. Loss: 1.0448\n",
            "Epoch: 4970/5000............. Loss: 1.0444\n",
            "Epoch: 4980/5000............. Loss: 1.0458\n",
            "Epoch: 4990/5000............. Loss: 1.0449\n",
            "Epoch: 5000/5000............. Loss: 1.0457\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20PRyK4OfPEv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "1b93baa8-03c2-40aa-f175-a606b893ad98"
      },
      "source": [
        "torch.save(rnn, 'shakespeare-rnn-generation.pt')"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:402: UserWarning: Couldn't retrieve source code for container of type RNN. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDFqCLFpXXQd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(model, character):\n",
        "    model\n",
        "    character = np.array([[char2int[c] for c in character]])\n",
        "    character = one_hot_encode(character, dict_size, character.shape[1], 1)\n",
        "\n",
        "    character = torch.from_numpy(character).cuda()\n",
        "    \n",
        "    out, hidden = model(character)\n",
        "    out\n",
        "    prob = nn.functional.softmax(out[-1], dim=0).data\n",
        "    char_ind = torch.max(prob, dim=0)[1].item()\n",
        "\n",
        "    return int2char[char_ind], hidden"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWnoA7rrahB9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sample(model, out_len, start):\n",
        "    model\n",
        "    model.eval() # eval mode\n",
        "    start = start.lower()\n",
        "    chars = [ch for ch in start]\n",
        "    size = out_len - len(chars)\n",
        "    for ii in range(size):\n",
        "        char, h = predict(model, chars)\n",
        "        # if char == '-':\n",
        "        #     char=\"\\n\"\n",
        "        chars.append(char)\n",
        "\n",
        "    return ''.join(chars)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jsGwkCWSh3dh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "a83fdead-7a7e-408c-d260-dde00d6e9ae4"
      },
      "source": [
        "sample(rnn, 500, \"brave was he\")"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"brave was her than thou art the dear love you living sick of you, but not the disgrace, And summer's successify's day, To line, Then look into the beauty doth the time do I not muse, and make wire is thine and in my mind, So thou art strangely pass, By self-killed: That thou art thy mind's dead, Than you with thee that the world to stop parthese in thee and thine image strengths of mine own desert, And therefore to be must not be foes.    Those lips thou art thy mind's dead, Than you with thee t\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "loKVF2ZFetpz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}