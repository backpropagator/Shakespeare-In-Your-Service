{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Model.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "mqRpOPW_XXPk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import glob\n",
        "import os\n",
        "import unicodedata\n",
        "import string\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import random\n",
        "import time\n",
        "import math\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "import numpy as np"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2NuOgSAHfJDE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bf2cf32d-5bfd-484d-ebd7-b2a80b11689c"
      },
      "source": [
        "is_cuda = torch.cuda.is_available()\n",
        "\n",
        "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
        "if is_cuda:\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"GPU is available\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"GPU not available, CPU used\")"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU is available\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EPxlQa-9XXPr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "14c1f199-e03a-45af-bca3-753fc1f1cd3e"
      },
      "source": [
        "all_letters = string.ascii_letters + \" .,;:'!-?)(\"\n",
        "n_letters = len(all_letters)\n",
        "\n",
        "MAX_LENGTH = 15\n",
        "\n",
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "        and c in all_letters\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "def readfile(filename):\n",
        "    file = open(filename, encoding='utf-8').read().replace(\"\\n\",\" \")\n",
        "    return file\n",
        "\n",
        "data = readfile(\"/content/sample_data/Shakespere.txt\")\n",
        "#print(data)\n",
        "\n",
        "X = []\n",
        "y = []\n",
        "\n",
        "for i in range(0, len(data)-MAX_LENGTH):\n",
        "    X.append(data[i:i+MAX_LENGTH-1])\n",
        "    y.append(data[i+1:i+MAX_LENGTH])\n",
        "\n",
        "print(\"Number of Training Example: {}\".format(len(X)))\n",
        "print(\"\\nA Random Example:\")\n",
        "q = random.randint(0,len(X))\n",
        "print(\"Input: {}\\nOutput: {}\".format(X[q],y[q]))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of Training Example: 94198\n",
            "\n",
            "A Random Example:\n",
            "Input:  their habitat\n",
            "Output: their habitati\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "okY-sN6OXXPx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "int2char = dict(enumerate(all_letters))\n",
        "char2int = {c : i for i, c in int2char.items()}"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uP0iWGJyXXP2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "c6494d58-c1d5-4143-8ca9-bc3c7f9e0201"
      },
      "source": [
        "for i in range(len(X)):\n",
        "    X[i] = [char2int[c] for c in X[i]]\n",
        "    y[i] = [char2int[c] for c in y[i]]\n",
        "\n",
        "print(\"\\nA Random Example:\")\n",
        "q = random.randint(0,len(X))\n",
        "print(\"Input: {}\\nOutput: {}\".format(X[q],y[q]))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "A Random Example:\n",
            "Input: [52, 8, 13, 52, 4, 21, 4, 17, 24, 52, 1, 11, 4, 18]\n",
            "Output: [8, 13, 52, 4, 21, 4, 17, 24, 52, 1, 11, 4, 18, 18]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmlGgtKEXXP6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dict_size = len(char2int)\n",
        "seq_len = MAX_LENGTH-1\n",
        "batch_size = len(X)\n",
        "\n",
        "def one_hot_encode(sequence, dict_size, seq_len, batch_size):\n",
        "    features = np.zeros((batch_size, seq_len, dict_size), dtype=np.float32)\n",
        "    \n",
        "    for i in range(batch_size):\n",
        "        for u in range(seq_len):\n",
        "            #print(i, u, sequence[i][u])\n",
        "            features[i, u, sequence[i][u] ] = 1\n",
        "    return features"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmxIpLbGXXP-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = one_hot_encode(X, dict_size, seq_len, batch_size)\n",
        "#y = one_hot_encode(y, dict_size, seq_len, batch_size)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9puEJRLXXQG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = torch.from_numpy(X).cuda()\n",
        "y = torch.tensor(y).cuda()"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ogkjo_cKXXQO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, n_layers):\n",
        "        super(RNN,self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.n_layers = n_layers\n",
        "        \n",
        "        self.rnn = nn.RNN(input_size, hidden_size, n_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "    \n",
        "    def forward(self, input):\n",
        "        batch_size = input.size(0)\n",
        "        \n",
        "        hidden = self.initHidden(batch_size)\n",
        "        \n",
        "        out, hidden = self.rnn(input, hidden)\n",
        "        \n",
        "        out = out.contiguous().view(-1, self.hidden_size)\n",
        "        out = self.fc(out)\n",
        "        \n",
        "        return out, hidden\n",
        "    \n",
        "    def initHidden(self, batch_size):\n",
        "        return torch.zeros(self.n_layers, batch_size, self.hidden_size).cuda()"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qSWNlgvAXXQS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rnn = RNN(dict_size, 128, dict_size, n_layers=1)\n",
        "rnn.cuda()\n",
        "n_epochs = 2000\n",
        "lr = 0.001\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(rnn.parameters(), lr=lr)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uHETMMntXXQZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0928e504-5d08-4785-ea61-bca8b9caa61a"
      },
      "source": [
        "for epoch in range(1, n_epochs+1):\n",
        "    optimizer.zero_grad()\n",
        "    X.cuda()\n",
        "    y.cuda()\n",
        "    output, hidden = rnn(X)\n",
        "    #print(output)\n",
        "    loss = criterion( output, y.view(-1).long() )\n",
        "    #print(loss)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    if epoch%10 == 0:\n",
        "        print('Epoch: {}/{}.............'.format(epoch, n_epochs), end=' ')\n",
        "        print(\"Loss: {:.4f}\".format(loss.item()))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 10/2000............. Loss: 3.5079\n",
            "Epoch: 20/2000............. Loss: 3.1515\n",
            "Epoch: 30/2000............. Loss: 3.1007\n",
            "Epoch: 40/2000............. Loss: 3.0791\n",
            "Epoch: 50/2000............. Loss: 3.0638\n",
            "Epoch: 60/2000............. Loss: 3.0493\n",
            "Epoch: 70/2000............. Loss: 3.0333\n",
            "Epoch: 80/2000............. Loss: 3.0137\n",
            "Epoch: 90/2000............. Loss: 2.9898\n",
            "Epoch: 100/2000............. Loss: 2.9580\n",
            "Epoch: 110/2000............. Loss: 2.9168\n",
            "Epoch: 120/2000............. Loss: 2.8659\n",
            "Epoch: 130/2000............. Loss: 2.8141\n",
            "Epoch: 140/2000............. Loss: 2.7625\n",
            "Epoch: 150/2000............. Loss: 2.7126\n",
            "Epoch: 160/2000............. Loss: 2.6658\n",
            "Epoch: 170/2000............. Loss: 2.6186\n",
            "Epoch: 180/2000............. Loss: 2.5748\n",
            "Epoch: 190/2000............. Loss: 2.5331\n",
            "Epoch: 200/2000............. Loss: 2.4947\n",
            "Epoch: 210/2000............. Loss: 2.4593\n",
            "Epoch: 220/2000............. Loss: 2.4269\n",
            "Epoch: 230/2000............. Loss: 2.3989\n",
            "Epoch: 240/2000............. Loss: 2.3712\n",
            "Epoch: 250/2000............. Loss: 2.3466\n",
            "Epoch: 260/2000............. Loss: 2.3244\n",
            "Epoch: 270/2000............. Loss: 2.3039\n",
            "Epoch: 280/2000............. Loss: 2.2854\n",
            "Epoch: 290/2000............. Loss: 2.2679\n",
            "Epoch: 300/2000............. Loss: 2.2517\n",
            "Epoch: 310/2000............. Loss: 2.2365\n",
            "Epoch: 320/2000............. Loss: 2.2224\n",
            "Epoch: 330/2000............. Loss: 2.2090\n",
            "Epoch: 340/2000............. Loss: 2.1964\n",
            "Epoch: 350/2000............. Loss: 2.1843\n",
            "Epoch: 360/2000............. Loss: 2.1732\n",
            "Epoch: 370/2000............. Loss: 2.1630\n",
            "Epoch: 380/2000............. Loss: 2.1525\n",
            "Epoch: 390/2000............. Loss: 2.1430\n",
            "Epoch: 400/2000............. Loss: 2.1346\n",
            "Epoch: 410/2000............. Loss: 2.1259\n",
            "Epoch: 420/2000............. Loss: 2.1169\n",
            "Epoch: 430/2000............. Loss: 2.1087\n",
            "Epoch: 440/2000............. Loss: 2.1012\n",
            "Epoch: 450/2000............. Loss: 2.0934\n",
            "Epoch: 460/2000............. Loss: 2.0860\n",
            "Epoch: 470/2000............. Loss: 2.0792\n",
            "Epoch: 480/2000............. Loss: 2.0720\n",
            "Epoch: 490/2000............. Loss: 2.0652\n",
            "Epoch: 500/2000............. Loss: 2.0584\n",
            "Epoch: 510/2000............. Loss: 2.0525\n",
            "Epoch: 520/2000............. Loss: 2.0453\n",
            "Epoch: 530/2000............. Loss: 2.0392\n",
            "Epoch: 540/2000............. Loss: 2.0329\n",
            "Epoch: 550/2000............. Loss: 2.0266\n",
            "Epoch: 560/2000............. Loss: 2.0211\n",
            "Epoch: 570/2000............. Loss: 2.0148\n",
            "Epoch: 580/2000............. Loss: 2.0089\n",
            "Epoch: 590/2000............. Loss: 2.0029\n",
            "Epoch: 600/2000............. Loss: 1.9970\n",
            "Epoch: 610/2000............. Loss: 1.9914\n",
            "Epoch: 620/2000............. Loss: 1.9856\n",
            "Epoch: 630/2000............. Loss: 1.9802\n",
            "Epoch: 640/2000............. Loss: 1.9753\n",
            "Epoch: 650/2000............. Loss: 1.9691\n",
            "Epoch: 660/2000............. Loss: 1.9637\n",
            "Epoch: 670/2000............. Loss: 1.9585\n",
            "Epoch: 680/2000............. Loss: 1.9531\n",
            "Epoch: 690/2000............. Loss: 1.9480\n",
            "Epoch: 700/2000............. Loss: 1.9428\n",
            "Epoch: 710/2000............. Loss: 1.9377\n",
            "Epoch: 720/2000............. Loss: 1.9328\n",
            "Epoch: 730/2000............. Loss: 1.9277\n",
            "Epoch: 740/2000............. Loss: 1.9227\n",
            "Epoch: 750/2000............. Loss: 1.9179\n",
            "Epoch: 760/2000............. Loss: 1.9131\n",
            "Epoch: 770/2000............. Loss: 1.9082\n",
            "Epoch: 780/2000............. Loss: 1.9036\n",
            "Epoch: 790/2000............. Loss: 1.8989\n",
            "Epoch: 800/2000............. Loss: 1.8946\n",
            "Epoch: 810/2000............. Loss: 1.8898\n",
            "Epoch: 820/2000............. Loss: 1.8852\n",
            "Epoch: 830/2000............. Loss: 1.8806\n",
            "Epoch: 840/2000............. Loss: 1.8763\n",
            "Epoch: 850/2000............. Loss: 1.8720\n",
            "Epoch: 860/2000............. Loss: 1.8675\n",
            "Epoch: 870/2000............. Loss: 1.8632\n",
            "Epoch: 880/2000............. Loss: 1.8591\n",
            "Epoch: 890/2000............. Loss: 1.8547\n",
            "Epoch: 900/2000............. Loss: 1.8505\n",
            "Epoch: 910/2000............. Loss: 1.8464\n",
            "Epoch: 920/2000............. Loss: 1.8423\n",
            "Epoch: 930/2000............. Loss: 1.8382\n",
            "Epoch: 940/2000............. Loss: 1.8341\n",
            "Epoch: 950/2000............. Loss: 1.8303\n",
            "Epoch: 960/2000............. Loss: 1.8262\n",
            "Epoch: 970/2000............. Loss: 1.8224\n",
            "Epoch: 980/2000............. Loss: 1.8185\n",
            "Epoch: 990/2000............. Loss: 1.8146\n",
            "Epoch: 1000/2000............. Loss: 1.8108\n",
            "Epoch: 1010/2000............. Loss: 1.8071\n",
            "Epoch: 1020/2000............. Loss: 1.8034\n",
            "Epoch: 1030/2000............. Loss: 1.7996\n",
            "Epoch: 1040/2000............. Loss: 1.7960\n",
            "Epoch: 1050/2000............. Loss: 1.7925\n",
            "Epoch: 1060/2000............. Loss: 1.7889\n",
            "Epoch: 1070/2000............. Loss: 1.7852\n",
            "Epoch: 1080/2000............. Loss: 1.7818\n",
            "Epoch: 1090/2000............. Loss: 1.7784\n",
            "Epoch: 1100/2000............. Loss: 1.7750\n",
            "Epoch: 1110/2000............. Loss: 1.7716\n",
            "Epoch: 1120/2000............. Loss: 1.7684\n",
            "Epoch: 1130/2000............. Loss: 1.7651\n",
            "Epoch: 1140/2000............. Loss: 1.7619\n",
            "Epoch: 1150/2000............. Loss: 1.7586\n",
            "Epoch: 1160/2000............. Loss: 1.7553\n",
            "Epoch: 1170/2000............. Loss: 1.7522\n",
            "Epoch: 1180/2000............. Loss: 1.7493\n",
            "Epoch: 1190/2000............. Loss: 1.7461\n",
            "Epoch: 1200/2000............. Loss: 1.7431\n",
            "Epoch: 1210/2000............. Loss: 1.7401\n",
            "Epoch: 1220/2000............. Loss: 1.7371\n",
            "Epoch: 1230/2000............. Loss: 1.7342\n",
            "Epoch: 1240/2000............. Loss: 1.7316\n",
            "Epoch: 1250/2000............. Loss: 1.7285\n",
            "Epoch: 1260/2000............. Loss: 1.7258\n",
            "Epoch: 1270/2000............. Loss: 1.7230\n",
            "Epoch: 1280/2000............. Loss: 1.7202\n",
            "Epoch: 1290/2000............. Loss: 1.7175\n",
            "Epoch: 1300/2000............. Loss: 1.7148\n",
            "Epoch: 1310/2000............. Loss: 1.7122\n",
            "Epoch: 1320/2000............. Loss: 1.7096\n",
            "Epoch: 1330/2000............. Loss: 1.7074\n",
            "Epoch: 1340/2000............. Loss: 1.7045\n",
            "Epoch: 1350/2000............. Loss: 1.7019\n",
            "Epoch: 1360/2000............. Loss: 1.6995\n",
            "Epoch: 1370/2000............. Loss: 1.6970\n",
            "Epoch: 1380/2000............. Loss: 1.6946\n",
            "Epoch: 1390/2000............. Loss: 1.6922\n",
            "Epoch: 1400/2000............. Loss: 1.6900\n",
            "Epoch: 1410/2000............. Loss: 1.6875\n",
            "Epoch: 1420/2000............. Loss: 1.6852\n",
            "Epoch: 1430/2000............. Loss: 1.6828\n",
            "Epoch: 1440/2000............. Loss: 1.6805\n",
            "Epoch: 1450/2000............. Loss: 1.6784\n",
            "Epoch: 1460/2000............. Loss: 1.6763\n",
            "Epoch: 1470/2000............. Loss: 1.6739\n",
            "Epoch: 1480/2000............. Loss: 1.6716\n",
            "Epoch: 1490/2000............. Loss: 1.6695\n",
            "Epoch: 1500/2000............. Loss: 1.6673\n",
            "Epoch: 1510/2000............. Loss: 1.6652\n",
            "Epoch: 1520/2000............. Loss: 1.6631\n",
            "Epoch: 1530/2000............. Loss: 1.6616\n",
            "Epoch: 1540/2000............. Loss: 1.6591\n",
            "Epoch: 1550/2000............. Loss: 1.6570\n",
            "Epoch: 1560/2000............. Loss: 1.6549\n",
            "Epoch: 1570/2000............. Loss: 1.6529\n",
            "Epoch: 1580/2000............. Loss: 1.6509\n",
            "Epoch: 1590/2000............. Loss: 1.6489\n",
            "Epoch: 1600/2000............. Loss: 1.6469\n",
            "Epoch: 1610/2000............. Loss: 1.6450\n",
            "Epoch: 1620/2000............. Loss: 1.6431\n",
            "Epoch: 1630/2000............. Loss: 1.6413\n",
            "Epoch: 1640/2000............. Loss: 1.6397\n",
            "Epoch: 1650/2000............. Loss: 1.6375\n",
            "Epoch: 1660/2000............. Loss: 1.6356\n",
            "Epoch: 1670/2000............. Loss: 1.6337\n",
            "Epoch: 1680/2000............. Loss: 1.6319\n",
            "Epoch: 1690/2000............. Loss: 1.6301\n",
            "Epoch: 1700/2000............. Loss: 1.6283\n",
            "Epoch: 1710/2000............. Loss: 1.6266\n",
            "Epoch: 1720/2000............. Loss: 1.6248\n",
            "Epoch: 1730/2000............. Loss: 1.6234\n",
            "Epoch: 1740/2000............. Loss: 1.6213\n",
            "Epoch: 1750/2000............. Loss: 1.6196\n",
            "Epoch: 1760/2000............. Loss: 1.6180\n",
            "Epoch: 1770/2000............. Loss: 1.6162\n",
            "Epoch: 1780/2000............. Loss: 1.6146\n",
            "Epoch: 1790/2000............. Loss: 1.6130\n",
            "Epoch: 1800/2000............. Loss: 1.6115\n",
            "Epoch: 1810/2000............. Loss: 1.6097\n",
            "Epoch: 1820/2000............. Loss: 1.6081\n",
            "Epoch: 1830/2000............. Loss: 1.6065\n",
            "Epoch: 1840/2000............. Loss: 1.6049\n",
            "Epoch: 1850/2000............. Loss: 1.6033\n",
            "Epoch: 1860/2000............. Loss: 1.6025\n",
            "Epoch: 1870/2000............. Loss: 1.6008\n",
            "Epoch: 1880/2000............. Loss: 1.5988\n",
            "Epoch: 1890/2000............. Loss: 1.5973\n",
            "Epoch: 1900/2000............. Loss: 1.5958\n",
            "Epoch: 1910/2000............. Loss: 1.5943\n",
            "Epoch: 1920/2000............. Loss: 1.5928\n",
            "Epoch: 1930/2000............. Loss: 1.5914\n",
            "Epoch: 1940/2000............. Loss: 1.5900\n",
            "Epoch: 1950/2000............. Loss: 1.5886\n",
            "Epoch: 1960/2000............. Loss: 1.5871\n",
            "Epoch: 1970/2000............. Loss: 1.5858\n",
            "Epoch: 1980/2000............. Loss: 1.5855\n",
            "Epoch: 1990/2000............. Loss: 1.5831\n",
            "Epoch: 2000/2000............. Loss: 1.5818\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20PRyK4OfPEv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "3c98c5b7-0d1c-473b-c15a-5402956e83df"
      },
      "source": [
        "torch.save(rnn, 'shakespeare-rnn-generation.pt')"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:402: UserWarning: Couldn't retrieve source code for container of type RNN. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDFqCLFpXXQd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(model, character):\n",
        "    model\n",
        "    character = np.array([[char2int[c] for c in character]])\n",
        "    character = one_hot_encode(character, dict_size, character.shape[1], 1)\n",
        "\n",
        "    character = torch.from_numpy(character).cuda()\n",
        "    \n",
        "    out, hidden = model(character)\n",
        "    out\n",
        "    prob = nn.functional.softmax(out[-1], dim=0).data\n",
        "    char_ind = torch.max(prob, dim=0)[1].item()\n",
        "\n",
        "    return int2char[char_ind], hidden"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWnoA7rrahB9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sample(model, out_len, start):\n",
        "    model\n",
        "    model.eval() # eval mode\n",
        "    start = start.lower()\n",
        "    chars = [ch for ch in start]\n",
        "    size = out_len - len(chars)\n",
        "    for ii in range(size):\n",
        "        char, h = predict(model, chars)\n",
        "        # if char == '-':\n",
        "        #     char=\"\\n\"\n",
        "        chars.append(char)\n",
        "\n",
        "    return ''.join(chars)"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jsGwkCWSh3dh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "86b6df4f-2fc0-4ccc-be96-9306b62089c7"
      },
      "source": [
        "sample(rnn, 200, \"Winter\")"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'winter that the world when the world when the world when the world when the world when the world when the world when the world when the world when the world when the world when the world when the worl'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "loKVF2ZFetpz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 41,
      "outputs": []
    }
  ]
}